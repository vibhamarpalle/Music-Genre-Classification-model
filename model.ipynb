{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Load the train data\n",
        "#train = pd.read_csv('/train.csv')"
      ],
      "metadata": {
        "id": "RMb7b_UCkN0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8YPYxBMkLqC"
      },
      "outputs": [],
      "source": [
        "# Encoding the labels\n",
        "\n",
        "encoded_labels = []\n",
        "for i in range(len(train)):\n",
        "    if train[\"label\"][i] == \"blues\":\n",
        "        encoded_labels.append(0)\n",
        "    if train[\"label\"][i] == \"classical\":\n",
        "        encoded_labels.append(1)\n",
        "    if train[\"label\"][i] == \"country\":\n",
        "        encoded_labels.append(2)\n",
        "    if train[\"label\"][i] == \"disco\":\n",
        "        encoded_labels.append(3)\n",
        "    if train[\"label\"][i] == \"hiphop\":\n",
        "        encoded_labels.append(4)\n",
        "    if train[\"label\"][i] == \"jazz\":\n",
        "        encoded_labels.append(5)\n",
        "    if train[\"label\"][i] == \"metal\":\n",
        "        encoded_labels.append(6)\n",
        "    if train[\"label\"][i] == \"pop\":\n",
        "        encoded_labels.append(7)\n",
        "    if train[\"label\"][i] == \"reggae\":\n",
        "        encoded_labels.append(8)\n",
        "    if train[\"label\"][i] == \"rock\":\n",
        "        encoded_labels.append(9)\n",
        "\n",
        "\n",
        "train[\"encoded_label\"] = encoded_labels\n",
        "\n",
        "# Extracting all the faetures from the dataset\n",
        "features = train.drop([\"filename\", \"label\", \"length\", \"encoded_label\"], axis = \"columns\")\n",
        "print(train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the features\n",
        "\n",
        "features_norm = pd.DataFrame()\n",
        "for column in features.columns:\n",
        "    features_norm[column] = (features[column] - features[column].mean()) / features[column].std()"
      ],
      "metadata": {
        "id": "AbBu7OQrkX4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding correlation between features\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (40, 40))\n",
        "sns.heatmap(features_norm.corr(), ax = ax, annot = True, linewidths = .5)"
      ],
      "metadata": {
        "id": "FHgv394ukb2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the distribution of different genres\n",
        "\n",
        "genre_counts = train['label'].value_counts()\n",
        "print(genre_counts)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=genre_counts.index, y=genre_counts.values)\n",
        "plt.title(\"Genre Distribution\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Genre\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l3iJsu_IkeZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing PCA to remove the correlated features and finding the most important features\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components = 57)\n",
        "features_final = pca.fit(features_norm)\n",
        "n_pcs = pca.components_.shape[0]\n",
        "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
        "initial_feature_names = list(features.columns)\n",
        "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
        "dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n",
        "\n",
        "df = pd.DataFrame(dic.items())\n",
        "\n",
        "\n",
        "final_features = []\n",
        "for i in range(len(df)):\n",
        "    if df[1][i] not in final_features:\n",
        "        final_features.append(df[1][i])\n",
        "final_features = final_features[:]\n",
        "train_final_pca = features_norm[final_features] # Removing correlated columns using PCA\n",
        "train_final_all = features_norm # Keeping the correlated columns\n",
        "train_final_no_mfcc2_mean = features_norm.drop([\"mfcc2_mean\"], axis = \"columns\") # Dropping only mfcc2_mean"
      ],
      "metadata": {
        "id": "kPQUhdoIkg6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing and fitting the KNN models\n",
        "\n",
        "import math\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn1 = KNeighborsClassifier(n_neighbors = 1,weights='distance',leaf_size= 50,algorithm='auto',p=1).fit(train_final_pca, train['encoded_label'])\n",
        "knn2 = KNeighborsClassifier(n_neighbors = 1,weights='distance',leaf_size= 50,algorithm='auto',p=1).fit(train_final_no_mfcc2_mean, train['encoded_label'])\n",
        "knn3 = KNeighborsClassifier(n_neighbors = 1,weights='distance',leaf_size= 50,algorithm='auto',p=1).fit(train_final_all, train['encoded_label'])"
      ],
      "metadata": {
        "id": "UB6yXIKBknJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the test dataset\n",
        "\n",
        "test = pd.read_csv('/kaggle/input/pesurrcampusmicompetitionc/test.csv')\n",
        "i_d = test['id']\n",
        "\n",
        "\n",
        "# Extracting features from the test dataset\n",
        "test_features = test.drop([\"length\", \"id\"], axis = \"columns\")\n",
        "\n",
        "# Normalizing the features\n",
        "test_features_norm = pd.DataFrame()\n",
        "for column in test_features.columns:\n",
        "    test_features_norm[column] = (test_features[column] - test_features[column].mean()) / test_features[column].std()\n",
        "\n",
        "\n",
        "test_final_pca = test_features_norm[final_features] # Removing correlated columns using PCA\n",
        "test_final_all = test_features_norm # Keeping the correlated columns (gives higher accuracy)\n",
        "test_final_no_mfcc2_mean = test_features_norm.drop([\"mfcc2_mean\"], axis = \"columns\") # Dropping only mfcc2_mean\n",
        "\n",
        "# Making predictions using the fitted KNN models\n",
        "predictions1 = knn1.predict(test_final_pca)\n",
        "predictions2 = knn2.predict(test_final_no_mfcc2_mean)\n",
        "predictions3 = knn3.predict(test_final_all)\n",
        "\n",
        "# Making a dataframe for the predictions and converting the dataframe into a csv file\n",
        "submission_df = pd.DataFrame({'id': i_d ,'label': predictions3})\n",
        "submission_df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "P-QT5J8BkoZL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}